{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 트랜스포머로 만드는 대화형 챗봇 [프로젝트]\n",
    "\n",
    "## 10-1. 프로젝트: 한국어 데이터로 챗봇 만들기\n",
    "\n",
    "영어로 만들었던 챗봇을 한국어 데이터로 바꿔서 훈련시켜봅시다.\n",
    "\n",
    "시작하기 전에 우선 주요 라이브러리 버전을 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 수집하기\n",
    "\n",
    "한국어 챗봇 데이터는 송영숙님이 공개한 챗봇 데이터를 사용합니다.\n",
    "\n",
    "이 데이터는 아래의 링크에서 다운로드할 수 있습니다.\n",
    "\n",
    "songys/Chatbot_data\n",
    "\n",
    "https://github.com/songys/Chatbot_data/blob/master/ChatbotData.csv\n",
    "\n",
    "Cloud shell에서 아래 명령어를 입력해 주세요.\n",
    "\n",
    "```bash\n",
    "!mkdir -p aiffel/transformer_chatbot/data/\n",
    "!ln -s ~/data/* ~/aiffel/transformer_chatbot/data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 전처리하기\n",
    "\n",
    "영어 데이터와는 전혀 다른 데이터인 만큼 영어 데이터에 사용했던 전처리와 일부 동일한 전처리도 필요하겠지만 전체적으로는 다른 전처리를 수행해야 할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Add space before punctuation\n",
    "    text = re.sub(r\"([?.!,])\", r\" \\1 \", text)\n",
    "    # Replace all characters except \".,!?\" with space\n",
    "    text = re.sub(r' +', \" \", text)\n",
    "    return text\n",
    "\n",
    "def preprocess_csv_file(input_path, output_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    data = pd.read_csv(input_path)\n",
    "    \n",
    "    # Apply preprocessing on 'Q' and 'A' columns\n",
    "    data['Q'] = data['Q'].apply(preprocess_text)\n",
    "    data['A'] = data['A'].apply(preprocess_text)\n",
    "\n",
    "    # Drop the 'label' column\n",
    "    data = data.drop(columns=['label'])\n",
    "    \n",
    "    # Save the preprocessed data to output file\n",
    "    data.to_csv(output_path, index=False)\n",
    "\n",
    "# Usage example:\n",
    "input_filepath = 'aiffel/transformer_chatbot/data/ChatbotData.csv'\n",
    "output_filepath = 'aiffel/transformer_chatbot/preprocessed/ChatbotData.csv'\n",
    "preprocess_csv_file(input_filepath, output_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. SubwordTextEncoder 사용하기\n",
    "\n",
    "한국어 데이터는 형태소 분석기를 사용하여 토크나이징을 해야 한다고 많은 분이 알고 있습니다. 하지만 여기서는 형태소 분석기가 아닌 위 실습에서 사용했던 내부 단어 토크나이저인 SubwordTextEncoder를 그대로 사용해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. 모델 구성하기\n",
    "\n",
    "위 실습 내용을 참고하여 트랜스포머 모델을 구현합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. 모델 평가하기\n",
    "\n",
    "Step 1에서 선택한 전처리 방법을 고려하여 입력된 문장에 대해서 대답을 얻는 예측 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
